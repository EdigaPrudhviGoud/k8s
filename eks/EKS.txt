```
EKS Pod Identity vs IRSA:

Feature	                              IRSA	                    EKS Pod Identity
Setup complexity	                  Higher	                Lower
Requires OIDC provider	              ✅ Yes	                ❌ No
Fine-grained permissions	          ✅ Yes	                ✅ Yes
Native EKS integration	              ❌ Not deeply integrated	✅ Fully integrated
Automatic credential injection	      ❌ No	                    ✅ Yes
```

Prerequisites: kubectl,eksctl,awscli

Fargate Profiles: 
Click Add fargate profile -> 

1.Install AWSCLI
AWS CLI -The open-source AWS CLI is a really powerful resource. This enables our interaction with the entire AWS service. Just by doing a few simple steps, we may operate our AWS account from anywhere.
Prerequisite -1.AWS Account    2.IAM User     3.Access key ID and Secret access key
Verification: aws --version

2.aws configure - IAM User details which have required roles 
Enter Access Key ID: <key>
Enter Secret Access key: <key>
Default region name:  <name>
Default format: json

3.Install kubectl
The Kubernetes API server can be reached using the command line program kubectl. When we access our AWS EKS cluster from our system, Kubectl is a big help. We can use our machine to access all of Kubernete's resources. Many operating system package managers contain the kubectl binary. It’s frequently simpler to use a package manager for installation than to download and manually install everything.
Verification: kubectl version 

4.Install EKSCTL(developed by weaveworks)
EKSCTL -The AWS EKS cluster can be interacted with using the tool EKSCTL. We’ll carry out the operations necessary to create, update, manage, and delete the AWS EKS cluster. This command line interface for communicating with the AWS EKS Cluster is highly useful.
Verification: eksctl version

5.eksctl needs to authenticate with AWS 
cat ~/.aws/credentials
aws_access_key_id=xxxxxxxxxxxxxxx
aws_secret_access_key=xxxxxxxxxxx

EKS doesn't manage worker nodes, we need to setup the worker nodes
1.Self-Managed Nodes
2.Managed Node Group
3.Fargate - Severless architecture, Will create worker nodes on demand, No need to provision/maintain EC2, automatically select optimal EC2 sizing

Creating an EKS cluster:
i)Cluster name, k8s version
ii)IAM role for cluster - provisioning nodes, storage, secrets
iii)select VPC and subnets
iV)SG for cluster

Create worker nodes:
i)Create node group
ii)select EC2 type
iii)Define min/max no. of nodes
iv)Specify EKS cluster to which the worker nodes will connect

Connect to cluster

6.eksctl create cluster --help
    --name <>
    --version <>
    --region <>
    --nodegroup-name <>
    --node-type t2.micro
    --nodes 2 
 
Description: nodes=How many worker nodes  

ii)eksctl create cluster 
        --name <>
        --region <>
        --fargate

7.eksctl delete cluster --name <>

8.kubectl config view

Service -> 1.Cluster IP - Cluster level
           2.Nodeport - Node Level
           3.Load Balancer - Creates EIP address so that Public can access which is costly
Ingress - route traffic inside the cluster

************************************************************
***)Change cluster of EKS
aws eks update-kubeconfig --name <cluster-name> --region <region>

Fargate Profile: Defines which pods can run on Fargate based on namespace and labels. You can specify a namespace for your pods and any labels that must be matched.
Namespaces: Fargate profiles are tied to Kubernetes namespaces, allowing you to control which workloads run on Fargate.
Labels: You can filter pods that should run on Fargate by specifying certain labels. This helps in organizing workloads effectively.

***)Create Fargate profile
eksctl create fargateprofile \
    --cluster <cluster-name> \
    --region us-east-1 \
    --name <fargate-profile-name> \
    --namespace game-2048

***)For kind: Ingress we use ingressClassName=alb and alb ingress annotations
        ingressClassName : alb
********************************************************************************************************************************************************
https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/examples/2048/2048_full.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: game-2048
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: game-2048
  name: deployment-2048
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: app-2048
  replicas: 5
  template:
    metadata:
      labels:
        app.kubernetes.io/name: app-2048
    spec:
      containers:
      - image: public.ecr.aws/l6m2t8p7/docker-2048:latest
        imagePullPolicy: Always
        name: app-2048
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  namespace: game-2048
  name: service-2048
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  type: NodePort
  selector:
    app.kubernetes.io/name: app-2048
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  namespace: game-2048
  name: ingress-2048
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: service-2048
              port:
                number: 80
******************************************************************************************************************************************************
***)B4 creating alb controller - configure IAM OIDC provider
Configuring an IAM OIDC (OpenID Connect) provider is essential before creating an AWS ALB (Application Load Balancer) controller because it allows the controller to authenticate and authorize requests using IAM roles. Here are the main reasons for this configuration:
Security: The OIDC provider enables the ALB controller to verify the identity of users and services. This helps secure your application by ensuring that only authenticated users can access certain resources.

Steps:
    export cluster_name=demo-cluster
    oidc_id=$(aws eks describe-cluster --name $cluster_name --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5) 

Check if there is an IAM OIDC provider configured already
    aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4\n
If not, run the below command
eksctl utils associate-iam-oidc-provider --cluster $cluster_name --approve
******************************************************************************************************************************************************************
How to setup alb add on
i) Download IAM policy:
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json

ii) Create IAM Policy:
aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json
iii) Create IAM Role: and attach IAM policy to that role
eksctl create iamserviceaccount \
  --cluster=<your-cluster-name> \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --attach-policy-arn=arn:aws:iam::<your-aws-account-id>:policy/AWSLoadBalancerControllerIAMPolicy \
  --approveDeploy ALB controller
iv) Add helm repo:
helm repo add eks https://aws.github.io/eks-charts
v) Update the repo:
helm repo update eks
vi) Install:
helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system \ 
  --set clusterName=<your-cluster-name> \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=<region> \
  --set vpcId=<your-vpc-id>
vii) Verify that the deployments are running.
kubectl get deployment -n kube-system aws-load-balancer-controller







