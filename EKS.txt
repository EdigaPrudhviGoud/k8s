1.Install AWSCLI
AWS CLI -The open-source AWS CLI is a really powerful resource. This enables our interaction with the entire AWS service. Just by doing a few simple steps, we may operate our AWS account from anywhere.
Prerequisite -1.AWS Account    2.IAM User     3.Access key ID and Secret access key
Verification: aws --version

2.aws configure
Enter Access Key ID: <key>
Enter Secret Access key: <key>
Default region name:  <name>
Default format: json

3.Install kubectl
The Kubernetes API server can be reached using the command line program kubectl. When we access our AWS EKS cluster from our system, Kubectl is a big help. We can use our machine to access all of Kubernete's resources. Many operating system package managers contain the kubectl binary. It’s frequently simpler to use a package manager for installation than to download and manually install everything.
Verification: kubectl version 

4.Install EKSCTL(developed by weaveworks)
EKSCTL -The AWS EKS cluster can be interacted with using the tool EKSCTL. We’ll carry out the operations necessary to create, update, manage, and delete the AWS EKS cluster. This command line interface for communicating with the AWS EKS Cluster is highly useful.
Verification: eksctl version

5.eksctl needs to authenticate with AWS 
cat ~/.aws/credentials
aws_access_key_id=xxxxxxxxxxxxxxx
aws_secret_access_key=xxxxxxxxxxx

EKS doesn't manage worker nodes, we need to setup the worker nodes
1.Self-Managed Nodes
2.Managed Node Group
3.Fargate - Severless architecture, Will create worker nodes on demand, No need to provision/maintain EC2, automatically select optimal EC2 sizing

Creating an EKS cluster:
i)Cluster name, k8s version
ii)IAM role for cluster - provisioning nodes, storage, secrets
iii)select VPC and subnets
iV)SG for cluster

Create worker nodes:
i)Create node group
ii)select EC2 type
iii)Define min/max no. of nodes
iv)Specify EKS cluster to which the worker nodes will connect

Connect to cluster

6.eksctl create cluster 
    --name <>
    --version <>
    --region <>
    --nodegroup-name <>
    --node-type t2.micro
    --nodes 2 
Description: nodes=How many worker nodes  

7.eksctl delete cluster --name <>
